<div class="sub_title">Projects</div>
<div class="info_content">

    <div class="project">
        <div class="project_title">Tornado</div>
        <div class="project_content">
            <div class="project_img"></div>
            <div class="project_dsc">Tornado is a framework for online learning and data stream mining - developed in Python. The framework includes various incremental and online learning algorithms as well as concept drift detection methods. The framework was used in our research works published at well-known conferences and a jouranl. Tornado is publicly available on GitHub.</div>
        </div>
        <div class="project_source">[<a href="https://github.com/alipsgh/tornado">GitHub</a>] [<a href="http://www.iwera.ir/~ali/papers/ecml2016.pdf">ECML16</a>] [<a href="https://arxiv.org/abs/1710.02030">IJCNN18</a>] [<a href="https://link.springer.com/article/10.1007/s10994-018-5719-z">ML Journal</a>]</div>
    </div>

    <div class="project">
        <div class="project_title">One Single BiLSTM for Word Sense Disambiguation</div>
        <div class="project_content">
            <div class="project_img"></div>
            <div class="project_dsc">Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models. This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data.</div>
        </div>
        <div class="project_source">[<a href="https://github.com/iwera-git/deepBioWSD">GitHub</a>] [<a href="">arXiv</a>] [<a href="https://arxiv.org/pdf/1802.09059.pdf">Springer</a>]</div>
    </div>

</div>